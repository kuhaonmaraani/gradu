{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP=False           # false skippa esimerkkidatan latauksen\n",
    "ORGANIZE=False       # true luo nls dataframen\n",
    "CREATEFINAL=False    # true luo final dataframen  \n",
    "CALCULATECORR=False  # true laskee finaldf f statistiikat\n",
    "PRINT=False          # true tulostaa klusteriplotit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SKIP:\n",
    "    path = 'E:/Koulu/data/muu/Alberding_Pasila/pasivrs01-2122-2-checkInit'\n",
    "    # path = 'E:/Koulu/data/muu/Alberding_joensuu/MJOE-vrs-2275-5-checkInit'\n",
    "    pasdir = 'E:/Koulu/data/muu/Alberding_Pasila'\n",
    "    joedir = 'E:/Koulu/data/muu/Alberding_joensuu'\n",
    "\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "    df['datetime'] = pd.to_datetime(df['Event Time [UTC]']).dt.round('5min')\n",
    "\n",
    "    # df = df.drop(columns=['startDelay', 'TTFA[s]', 'Sigma HDOP', \n",
    "    #                         'Sigma H', 'Sigma NE', 'Epochs', 'Data-Age[s]', 'checkType',\n",
    "    #                       'Sigma Sat.', 'Sigma Data-Age', 'Sigma N', 'Sigma E', \n",
    "    #                       '# of Sat.', 'initTyp', 'Solution', 'Event Time [UTC]'])\n",
    "\n",
    "    df = df[['datetime', 'dN[cm]', 'dE[cm]', 'dNE[cm]', 'HDOP', 'Sigma NE', 'Sigma N', 'Sigma E']]\n",
    "\n",
    "    df = df.loc[(df['datetime'].dt.time >= dt.time(6)) & (df['datetime'].dt.time <= dt.time(16))]\n",
    "    df = df.groupby('datetime').mean().reset_index()\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime     dN[cm]     dE[cm]    dNE[cm]  HDOP   asema\n",
      "0 2019-01-16 06:10:00  21.500000  23.500000  31.800000   0.4  pasila\n",
      "1 2019-01-16 06:30:00  12.000000  28.000000  30.500000   0.4  pasila\n",
      "2 2019-01-16 06:45:00  33.200000  14.400000  36.200000   0.4  pasila\n",
      "3 2019-01-16 06:50:00  37.700000   2.600000  37.800000   0.4  pasila\n",
      "4 2019-01-16 06:55:00  35.000000  -1.225000  35.100000   0.4  pasila\n",
      "5 2019-01-16 07:00:00  38.033333  -3.566667  38.200000   0.4  pasila\n",
      "6 2019-01-16 07:05:00  47.566667 -10.400000  48.733333   0.4  pasila\n",
      "7 2019-01-16 07:10:00  62.625000 -12.850000  63.975000   0.4  pasila\n",
      "8 2019-01-16 07:15:00  64.533333 -17.066667  66.766667   0.4  pasila\n",
      "9 2019-01-16 07:20:00  70.233333  -7.033333  70.633333   0.4  pasila\n"
     ]
    }
   ],
   "source": [
    "ORGANIZE = False\n",
    "pasila = []\n",
    "pasiladates = []\n",
    "joensuu = []\n",
    "joensuudates = []\n",
    "fails = []\n",
    "\n",
    "dates = pd.read_csv('data/dates.csv')\n",
    "dates = pd.to_datetime(dates['date']).dt.date.values\n",
    "\n",
    "if ORGANIZE:    \n",
    "    i = 1\n",
    "    nfiles = len(os.listdir(pasdir))\n",
    "\n",
    "    nlsdf = pd.DataFrame(columns=['datetime', 'dN[cm]', 'dE[cm]', 'dNE[cm]', 'HDOP', 'Sigma NE', 'Sigma N', 'Sigma E', 'asema'])\n",
    "    \n",
    "    print('Processing Pasila files...')\n",
    "    for file in os.listdir(pasdir):\n",
    "        print(f'Processing file {file}, {i}/{nfiles}, {100*(i/nfiles):.2f}%', end='\\r')\n",
    "        i += 1\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(pasdir, file), delimiter=';')\n",
    "            date = pd.to_datetime(df['Event Time [UTC]'][0]).date()\n",
    "            if date in dates:\n",
    "                pasila.append(str(file))\n",
    "                pasiladates.append(date)\n",
    "\n",
    "                df['datetime'] = pd.to_datetime(df['Event Time [UTC]']).dt.round('5min')\n",
    "\n",
    "                \n",
    "                # Define expected columns\n",
    "                expected_cols = ['datetime', 'dN[cm]', 'dE[cm]', 'dNE[cm]', 'HDOP', 'Sigma NE', 'Sigma N', 'Sigma E']\n",
    "\n",
    "                # Add missing columns with NaN\n",
    "                for col in expected_cols:\n",
    "                    if col not in df.columns:\n",
    "                        df[col] = pd.NA\n",
    "\n",
    "                # Select only the expected columns\n",
    "                df = df[expected_cols]\n",
    "\n",
    "                \n",
    "                df = df.loc[(df['datetime'].dt.time >= dt.time(6)) & (df['datetime'].dt.time <= dt.time(16))]\n",
    "                df = df.groupby('datetime').mean().reset_index()\n",
    "                df['asema'] = 'pasila'\n",
    "\n",
    "                nlsdf = pd.concat([nlsdf, df], ignore_index=True)\n",
    "    \n",
    "        except:\n",
    "            fails.append(pasdir+'/'+str(file))\n",
    "    \n",
    "    i = 1\n",
    "    nfiles = len(os.listdir(joedir))\n",
    "    print('Processing Joensuu files...')\n",
    "    for file in os.listdir(joedir):\n",
    "        print(f'Processing file {file}, {i}/{nfiles}, {100*(i/nfiles):.2f}%', end='\\r')\n",
    "        i += 1\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(joedir, file), delimiter=';')\n",
    "            date = pd.to_datetime(df['Event Time [UTC]'][0]).date()\n",
    "            if date in dates:\n",
    "                joensuu.append(str(file))\n",
    "                joensuudates.append(date)\n",
    "\n",
    "                df['datetime'] = pd.to_datetime(df['Event Time [UTC]']).dt.round('5min')\n",
    "                \n",
    "                # Define expected columns\n",
    "                expected_cols = ['datetime', 'dN[cm]', 'dE[cm]', 'dNE[cm]', 'HDOP', 'Sigma NE', 'Sigma N', 'Sigma E']\n",
    "\n",
    "                # Add missing columns with NaN\n",
    "                for col in expected_cols:\n",
    "                    if col not in df.columns:\n",
    "                        df[col] = pd.NA\n",
    "\n",
    "                # Select only the expected columns\n",
    "                df = df[expected_cols]\n",
    "                \n",
    "                df = df.loc[(df['datetime'].dt.time >= dt.time(6)) & (df['datetime'].dt.time <= dt.time(16))]\n",
    "                df = df.groupby('datetime').mean().reset_index()\n",
    "                df['asema'] = 'joensuu'\n",
    "\n",
    "                nlsdf = pd.concat([nlsdf, df], ignore_index=True)\n",
    "    \n",
    "        except:\n",
    "            fails.append(joedir+'/'+str(file))\n",
    "    \n",
    "    nlsdf = nlsdf.sort_values('datetime')\n",
    "    nlsdf.to_csv('data/NLSdf.csv', index=False)\n",
    "\n",
    "    dif1 = set(dates).difference(pasiladates)\n",
    "    dif2 = set(dates).difference(joensuudates)\n",
    "\n",
    "    print(sorted(list(dif1&dif2)))\n",
    "    print()\n",
    "    print(fails)\n",
    "\n",
    "else:\n",
    "    nlsdf = pd.read_csv('data/NLSdf.csv')\n",
    "    nlsdf['datetime'] = pd.to_datetime(nlsdf['datetime'])\n",
    "    print(nlsdf.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime     dN[cm]     dE[cm]    dNE[cm]  HDOP  Sigma NE  \\\n",
      "0 2019-01-16 06:10:00  21.500000  23.500000  31.800000   0.4       NaN   \n",
      "1 2019-01-16 06:30:00  12.000000  28.000000  30.500000   0.4       NaN   \n",
      "2 2019-01-16 06:45:00  33.200000  14.400000  36.200000   0.4       NaN   \n",
      "3 2019-01-16 06:50:00  37.700000   2.600000  37.800000   0.4       NaN   \n",
      "4 2019-01-16 06:55:00  35.000000  -1.225000  35.100000   0.4       NaN   \n",
      "5 2019-01-16 07:00:00  38.033333  -3.566667  38.200000   0.4       NaN   \n",
      "6 2019-01-16 07:05:00  47.566667 -10.400000  48.733333   0.4       NaN   \n",
      "7 2019-01-16 07:10:00  62.625000 -12.850000  63.975000   0.4       NaN   \n",
      "8 2019-01-16 07:15:00  64.533333 -17.066667  66.766667   0.4       NaN   \n",
      "9 2019-01-16 07:20:00  70.233333  -7.033333  70.633333   0.4       NaN   \n",
      "\n",
      "   Sigma N  Sigma E   asema  k2  ...  k8  k9  k10  k15  k25  k30       date  \\\n",
      "0      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "1      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "2      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "3      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "4      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "5      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "6      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "7      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "8      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "9      NaN      NaN  pasila   1  ...   6   1    1    2    1   23 2019-01-16   \n",
      "\n",
      "     dNE_std  hairio  dNE_med  \n",
      "0  31.563175   False    62.55  \n",
      "1  31.563175   False    62.55  \n",
      "2  31.563175   False    62.55  \n",
      "3  31.563175   False    62.55  \n",
      "4  31.563175   False    62.55  \n",
      "5  31.563175   False    62.55  \n",
      "6  31.563175   False    62.55  \n",
      "7  31.563175   False    62.55  \n",
      "8  31.563175   False    62.55  \n",
      "9  31.563175   False    62.55  \n",
      "\n",
      "[10 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CREATEFINAL:\n",
    "\n",
    "    hairiopv = nlsdf.loc[(nlsdf['datetime'].dt.date == dt.date(2023,2,27))]\n",
    "    clusters = pd.read_csv('data/clusterdf.csv', sep=',')\n",
    "    clusters['dates'] = pd.to_datetime(clusters['dates'])\n",
    "\n",
    "    finaldf = pd.merge(nlsdf, clusters, how='left', left_on=nlsdf['datetime'].dt.date, right_on=clusters['dates'].dt.date)\n",
    "    finaldf = finaldf.drop(columns=['key_0', 'dates'])\n",
    "    finaldf[list(clusters.columns[1:])] = finaldf[list(clusters.columns[1:])].astype('category')\n",
    "    finaldf\n",
    "else:\n",
    "    finaldf = pd.read_csv('data/finaldf.csv', sep=';')\n",
    "    finaldf['datetime'] = pd.to_datetime(finaldf['datetime'])\n",
    "    finaldf['date'] = pd.to_datetime(finaldf['date'])\n",
    "    print(finaldf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dN[cm]</th>\n",
       "      <th>dE[cm]</th>\n",
       "      <th>dNE[cm]</th>\n",
       "      <th>HDOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dN[cm]</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220673</td>\n",
       "      <td>0.859692</td>\n",
       "      <td>-0.056711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dE[cm]</th>\n",
       "      <td>0.220673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.184712</td>\n",
       "      <td>-0.045678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dNE[cm]</th>\n",
       "      <td>0.859692</td>\n",
       "      <td>0.184712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDOP</th>\n",
       "      <td>-0.056711</td>\n",
       "      <td>-0.045678</td>\n",
       "      <td>-0.025928</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dN[cm]    dE[cm]   dNE[cm]      HDOP\n",
       "dN[cm]   1.000000  0.220673  0.859692 -0.056711\n",
       "dE[cm]   0.220673  1.000000  0.184712 -0.045678\n",
       "dNE[cm]  0.859692  0.184712  1.000000 -0.025928\n",
       "HDOP    -0.056711 -0.045678 -0.025928  1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlsdf.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATECORR:\n",
    "\n",
    "    from scipy.stats import f_oneway\n",
    "\n",
    "    for col in finaldf.columns:\n",
    "        if col.startswith('k'):\n",
    "            groups = [group['HDOP'].dropna().values for _, group in finaldf.groupby(col)]\n",
    "            if len(groups) > 1:\n",
    "                stat, p = f_oneway(*groups)\n",
    "                print(f\"{col}: F-statistic = {stat:.2f}, p-value = {p:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATECORR:    \n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    tukey = pairwise_tukeyhsd(endog=finaldf['HDOP'], groups=finaldf['k3'], alpha=0.05)\n",
    "    print(tukey.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mariav\\AppData\\Local\\anaconda3\\envs\\gradu\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: [106]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mariav\\AppData\\Local\\anaconda3\\envs\\gradu\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: [59 60 61 62]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/keog/\"\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "dataframes = []\n",
    "dates = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(data_dir, file))\n",
    "        dataframes.append(df)\n",
    "\n",
    "heatmaps = []  \n",
    "heatmaps_imputed = []  \n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "for df in dataframes:\n",
    "    XX, YY = np.meshgrid(df['time_seconds'].unique(), df['gdlat'].unique())\n",
    "    ZZ = df['blrmvd'].values\n",
    "    ZZ = np.reshape(ZZ, XX.T.shape).T\n",
    "\n",
    "    heatmaps.append(ZZ)\n",
    "\n",
    "    hmi = imputer.fit_transform(ZZ).flatten()\n",
    "    if hmi.size == 1125:\n",
    "        heatmaps_imputed.append(hmi)\n",
    "        first_date = df['datetime'].iloc[0][:10]\n",
    "        dates.append(first_date)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "heatmaps_imputed = np.vstack(heatmaps_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "\n",
    "if PRINT:\n",
    "    for i in range(k):\n",
    "        labels = np.array(clusters[f'k{k}'])\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        n_images = len(cluster_indices)\n",
    "        print(f\"Cluster {i}: {n_images} heatmaps\")\n",
    "\n",
    "        # Setup subplot grid\n",
    "        n_cols = 5\n",
    "        n_rows = math.ceil(n_images / n_cols)\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2*n_rows))\n",
    "        axes = axes.flatten()  # Flatten for easy indexing\n",
    "\n",
    "        for j, idx in enumerate(cluster_indices):\n",
    "            ax = axes[j]\n",
    "            ax.imshow(heatmaps[idx], aspect='auto', cmap='viridis')\n",
    "            ax.set_title(f\"{dates[idx]}\")\n",
    "            ax.axis('off')  # Hide axes for cleaner layout\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for b in range(j+1, len(axes)):\n",
    "            axes[b].axis('off')\n",
    "\n",
    "        fig.suptitle(f'Cluster {i}', fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for title\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
